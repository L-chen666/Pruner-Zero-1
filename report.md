# Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models
GitHub: https://github.com/pprp/Pruner-Zero
## 1. è®ºæ–‡æ€»ç»“
å°½ç®¡å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰å…·æœ‰å“è¶Šçš„åŠŸèƒ½ï¼Œä½†ç”±äºå…¶åºå¤§çš„è§„æ¨¡ï¼Œå®ƒä»¬é¢ä¸´ç€éƒ¨ç½²æŒ‘æˆ˜ã€‚ä¿®å‰ªæ–¹æ³•ä¼šé™ä½æƒé‡å­é›†ä»¥åŠ é€Ÿï¼Œä½†å…¶ä¸­è®¸å¤šæ–¹æ³•éœ€è¦é‡æ–°è®­ç»ƒï¼Œè¿™éå¸¸æ˜‚è´µä¸”è®¡ç®—é‡å¤§ã€‚æœ€è¿‘ï¼Œåè®­ç»ƒä¿®å‰ªæ–¹æ³•å¼•å…¥äº†æ–°çš„åº¦é‡ï¼Œä½¿å¾—LLMçš„ä¿®å‰ªæ— éœ€é‡æ–°è®­ç»ƒã€‚ç„¶è€Œï¼Œä¸ºäº†æœ‰æ•ˆåœ°è¯†åˆ«ä¸Šçº§å‰ªæåº¦é‡ï¼Œä½œè€…å¼€å‘äº†ä¸€ä¸ªä½¿ç”¨é—ä¼ ç¼–ç¨‹æœç´¢ç¬¦å·å‰ªæåº¦é‡çš„è‡ªåŠ¨æ¡†æ¶ã€‚ç‰¹åˆ«åœ°ï¼Œè¿˜è®¾è®¡äº†ä¸€ä¸ªåŒ…å«ç°æœ‰å‰ªæåº¦é‡çš„ç²¾ç»†æœç´¢ç©ºé—´æ¥å‘ç°æ½œåœ¨çš„ç¬¦å·å‰ªæåº¦é‡ï¼Œå¹¶æå‡ºäº†ä¸€ç§ç›¸åçš„æ“ä½œç®€åŒ–ç­–ç•¥æ¥å¢åŠ ç§ç¾¤çš„å¤šæ ·æ€§ï¼Œè¿™æ ·ï¼ŒåŸºäºæœç´¢ç»“æœï¼Œæœ¬æ–‡ç ”ç©¶äº†ç¬¦å·å‰ªæåº¦é‡ä¸å‰ªæåæ€§èƒ½ä¹‹é—´çš„å…³ç³»ï¼Œæ€»ç»“äº†ä¸€äº›åŸåˆ™ï¼Œå¹¶åœ¨LLaMAå’ŒLLaMA-2ä¸Šè¿›è¡Œäº†å¤§é‡çš„è¯­è¨€å»ºæ¨¡å’Œzero-shotä»»åŠ¡çš„å®éªŒï¼Œå®éªŒç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„PrunerZeroæ¯”SOTAåè®­ç»ƒå‰ªææ–¹æ³•å…·æœ‰ä¸Šçº§æ€§èƒ½ã€‚
## 2. è®ºæ–‡åˆ›æ–°ç‚¹
1. **è‡ªåŠ¨åŒ–çš„ç¬¦å·åŒ–å‰ªæåº¦é‡æœç´¢æ¡†æ¶**
 - **é¦–åˆ›æ€§**ï¼šPruner-Zeroæ˜¯é¦–ä¸ªåˆ©ç”¨é—ä¼ ç¼–ç¨‹ï¼ˆGenetic Programming, GPï¼‰ä»é›¶å¼€å§‹è‡ªåŠ¨æœç´¢ç¬¦å·åŒ–å‰ªæåº¦é‡ï¼ˆSymbolic Pruning Metric, SPMï¼‰çš„æ¡†æ¶ã€‚å®ƒé€šè¿‡è¿›åŒ–ç®—æ³•åŠ¨æ€ç”Ÿæˆå’Œä¼˜åŒ–å‰ªæåº¦é‡ï¼Œæ— éœ€äººå·¥è®¾è®¡å¤æ‚çš„å‰ªæè§„åˆ™ã€‚
 - **å…¨é¢çš„æœç´¢ç©ºé—´**ï¼šè¯¥æ¡†æ¶è®¾è®¡äº†ä¸€ä¸ªç»Ÿä¸€ä¸”å…¨é¢çš„æœç´¢ç©ºé—´ï¼Œæ¶µç›–äº†ç°æœ‰çš„å‰ªæåº¦é‡ï¼ˆå¦‚æƒé‡ã€æ¢¯åº¦ç­‰ï¼‰ï¼Œå¹¶å¼•å…¥äº†å¤šç§åŸºæœ¬æ“ä½œï¼ˆå¦‚åŠ æ³•ã€ä¹˜æ³•ã€å½’ä¸€åŒ–ç­‰ï¼‰ï¼Œèƒ½å¤Ÿé‡æ„å’Œä¼˜åŒ–ç°æœ‰çš„å‰ªææ–¹æ³•ã€‚
2. **å¯¹ç«‹æ“ä½œç®€åŒ–ç­–ç•¥ï¼ˆOpposing Operation Simplification, OOSï¼‰**
- **ä¼˜åŒ–æœç´¢æ•ˆç‡**ï¼šOOSç­–ç•¥é€šè¿‡è¯†åˆ«å’Œæ¶ˆé™¤ç¬¦å·æ ‘ä¸­å¯¹ç«‹çš„æ“ä½œï¼ˆå¦‚expå’Œlogã€sqrtå’Œsqrç­‰ï¼‰ï¼Œå‡å°‘æœç´¢ç©ºé—´ä¸­çš„å†—ä½™ï¼Œæé«˜æœç´¢æ•ˆç‡å’Œå‰ªæåº¦é‡çš„å¤šæ ·æ€§ã€‚
- **æå‡å‰ªææ€§èƒ½**ï¼šè¯¥ç­–ç•¥ä¸ä»…ç®€åŒ–äº†ç¬¦å·è¡¨è¾¾å¼ï¼Œè¿˜é€šè¿‡å‡å°‘å†—ä½™æ“ä½œï¼Œä½¿å¾—æœ€ç»ˆæœç´¢åˆ°çš„å‰ªæåº¦é‡æ›´åŠ ç®€æ´ä¸”æ€§èƒ½æ›´ä¼˜ã€‚
3. **æ— éœ€é‡æ–°è®­ç»ƒçš„é«˜æ•ˆå‰ªææ–¹æ³•**
- **æ— éœ€æƒé‡æ›´æ–°**ï¼šPruner-Zeroåœ¨å‰ªæè¿‡ç¨‹ä¸­æ— éœ€å¯¹æ¨¡å‹æƒé‡è¿›è¡Œæ›´æ–°æˆ–é‡æ–°è®­ç»ƒï¼Œæ˜¾è‘—é™ä½äº†è®¡ç®—æˆæœ¬å’Œèµ„æºéœ€æ±‚ï¼Œå°¤å…¶é€‚ç”¨äºå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ã€‚
- **å¿«é€Ÿè¯„ä¼°**ï¼šé€šè¿‡åœ¨LLaMA-2-7Bæ¨¡å‹ä¸Šè¿›è¡Œå¿«é€Ÿåå‰ªæè¯„ä¼°ï¼ˆæ¯æ¬¡è¯„ä¼°è€—æ—¶ä¸åˆ°5åˆ†é’Ÿï¼‰ï¼ŒPruner-Zeroèƒ½å¤Ÿåœ¨çŸ­æ—¶é—´å†…æ‰¾åˆ°é«˜æ•ˆçš„å‰ªæåº¦é‡ã€‚
4. **å¹¿æ³›çš„å®éªŒéªŒè¯å’Œæ€§èƒ½æå‡**
- **è¶…è¶Šç°æœ‰æ–¹æ³•**ï¼šåœ¨LLaMAå’ŒLLaMA-2æ¨¡å‹ä¸Šï¼ŒPruner-Zeroåœ¨å¤šç§å‰ªææ¯”ä¾‹ï¼ˆå¦‚50%ã€4:8ã€2:4ï¼‰ä¸‹å‡ä¼˜äºç°æœ‰çš„å‰ªææ–¹æ³•ï¼ˆå¦‚SparseGPTã€Wandaç­‰ï¼‰ï¼Œåœ¨è¯­è¨€å»ºæ¨¡å’Œé›¶æ ·æœ¬ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ›´ä½çš„å›°æƒ‘åº¦ï¼ˆPerplexityï¼‰å’Œæ›´é«˜çš„å‡†ç¡®ç‡ã€‚
- **é€‚ç”¨äºå¤šç§æ¨¡å‹æ¶æ„**ï¼šPruner-Zeroä¸ä»…åœ¨LLaMAç³»åˆ—æ¨¡å‹ä¸Šè¡¨ç°å‡ºè‰²ï¼Œè¿˜æˆåŠŸåº”ç”¨äºå…¶ä»–æ¨¡å‹ï¼ˆå¦‚Tiny-LLaMAå’ŒOPTï¼‰ï¼Œè¯æ˜äº†å…¶é€šç”¨æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚
## 3. æµç¨‹å›¾
<p align="center">
<img src="https://github.com/L-chen666/Pruner-Zero-1/blob/main/alt%20text.png" width=100% height=100% 
class="center">

```mermaid
graph TB
    %% --- æ ·å¼å®šä¹‰ ---
    classDef default font-family:'Segoe UI',Arial,sans-serif,font-size:14px;
    
    %% æ ¸å¿ƒè¿‡ç¨‹èŠ‚ç‚¹ (è“è‰²ç³»)
    classDef process fill:#e3f2fd,stroke:#1565c0,stroke-width:2px,rx:8,ry:8,color:#0d47a1;
    %% æ•°æ®/å¯¹è±¡èŠ‚ç‚¹ (é»„è‰²ç³»)
    classDef data fill:#fffde7,stroke:#fbc02d,stroke-width:2px,rx:4,ry:4,color:#f57f17;
    %% å¼€å§‹/ç»“æŸèŠ‚ç‚¹ (ç»¿è‰²ç³»)
    classDef start fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px,rx:20,ry:20,color:#1b5e20;
    %% è¯„ä¼°/æŒ‡æ ‡èŠ‚ç‚¹ (ç´«è‰²ç³»)
    classDef metric fill:#f3e5f5,stroke:#8e24aa,stroke-width:2px,stroke-dasharray: 5 5,rx:5,ry:5,color:#4a148c;
    %% å…³é”®åˆ›æ–°ç‚¹ (çº¢è‰²ç³»)
    classDef highlight fill:#ffebee,stroke:#c62828,stroke-width:3px,rx:8,ry:8,color:#b71c1c;

    %% --- 1. åˆå§‹åŒ– ---
    Init([ğŸš€ Initialization<br/>Symbolic Metric]):::start

    %% --- 2. è¿›åŒ–å¾ªç¯ ---
    subgraph EvoLoop [ğŸ§¬ Evolutionary Search Loop]
        direction TB
        style EvoLoop fill:#fafafa,stroke:#bdbdbd,stroke-width:2px,stroke-dasharray: 5 5,color:#616161
        
        Pop[ğŸ‘¥ Population]:::data
        Select[ğŸ† Selection<br/>Tournament]:::process
        Parents[ğŸ‘ª Parents]:::data
        Cross[ğŸ”€ Cross Over]:::process
        Mut[ğŸ§¬ Mutation]:::process
        Simp[âœ¨ Opposing Operation<br/>Simplification]:::highlight
        NewSym(ğŸ“ New Symbolic Metric):::data

        Pop --> Select
        Select --> Parents
        Parents --> Cross
        Cross --> Mut
        Mut --> Simp
        Simp --> NewSym
    end

    %% --- 3. è¯„ä¼° ---
    subgraph Eval [â±ï¸ Post-training Evaluation < 5 mins]
        direction TB
        style Eval fill:#f9fbe7,stroke:#afb42b,stroke-width:2px,color:#827717
        
        LLM[ğŸ§  Original LLM]:::data
        Pruned[âœ‚ï¸ Pruned LLM]:::data
        Calc{âš™ï¸ Apply Metric}:::process
        Score[ğŸ“Š Perplexity<br/>Wikitext2 / One-shot]:::metric

        LLM --> Calc
        Calc --> Pruned
        Pruned --> Score
    end

    %% --- è¿æ¥å…³ç³» ---
    Init ==> Pop
    NewSym ==> Calc
    Score == "Add to Population" ==> Pop

    %% è°ƒæ•´è¿çº¿æ ·å¼
    linkStyle default stroke:#546e7a,stroke-width:2px,fill:none;
```

 ## 4.å…¬å¼ä¸ä»£ç å¯¹åº”è¡¨

### 4.1 æ ¸å¿ƒå…¬å¼æ¦‚è§ˆè¡¨

| å…¬å¼åç§° / æè¿° | æ•°å­¦å…¬å¼ (è¿‘ä¼¼è¡¨ç¤º) | æ–‡ä»¶å | è¡Œå· |
| :--- | :--- | :--- | :--- |
| **Hessian çŸ©é˜µåœ¨çº¿æ›´æ–°** (SparseGPT) | $$H_{new} = \frac{n}{n+\Delta n} H_{old} + \sqrt{\frac{2}{n+\Delta n}} X X^T$$ | `lib/sparsegpt.py` | 35-38 |
| **Hessian é€†çŸ©é˜µè®¡ç®—** (Cholesky) | $$H^{-1} = (L L^T)^{-1}$$ | `lib/sparsegpt.py` | 64-67 |
| **æ˜¾è‘—æ€§åˆ†æ•° / å‰ªææŒ‡æ ‡** (OBS Metric) | $$\text{metric} = \frac{w^2}{([H^{-1}]_{ii})^2}$$ | `lib/sparsegpt.py` | 84 |
| **å›°æƒ‘åº¦è®¡ç®—** (Perplexity) | $$PPL = \exp\left(\frac{1}{N} \sum -\log P(x_i)\right)$$ | `lib/eval.py` | 75 |
| **è´Ÿå¯¹æ•°ä¼¼ç„¶** (NLL) | $$\text{NLL} = \text{CrossEntropy} \times \text{SeqLen}$$ | `lib/eval.py` | 66-70 |
| **Min-Max å½’ä¸€åŒ–ç®—å­** (MMS) | $$f(x) = \frac{x - \min(x)}{\max(x) - \min(x)}$$ | `lib/gptree.py` | 99 |
| **Z-Score å½’ä¸€åŒ–ç®—å­** (ZSN) | $$f(x) = \frac{x - \mu}{\sigma}$$ | `lib/gptree.py` | 107 |
| **é™¤æ³•ç®—å­ (å¸¦å½’ä¸€åŒ–)** (Div) | $$f(x, y) = \frac{x}{\|y\|_2}$$ | `lib/gptree.py` | 37 |
| **æ¨¡å‹ç¨€ç–åº¦è®¡ç®—** | $$\text{Sparsity} = \frac{\sum \mathbb{I}(w=0)}{N_{total}}$$ | `lib/prune.py` | 49-58 |

---

### 4.2 ä»£ç å®ç°

### Hessian çŸ©é˜µåœ¨çº¿æ›´æ–°
**æ–‡ä»¶**: `lib/sparsegpt.py`
**è¡Œå·**: 35-38
ä»£ç ä½¿ç”¨ç´¯ç§¯æ›´æ–°çš„æ–¹å¼è¿‘ä¼¼ Hessian çŸ©é˜µï¼š
```python
self.H *= self.nsamples / (self.nsamples + tmp)
self.nsamples += tmp
inp = math.sqrt(2 / self.nsamples) * inp.float()
self.H += inp.matmul(inp.t())
```

### Hessian é€†çŸ©é˜µè®¡ç®—
**æ–‡ä»¶**: `lib/sparsegpt.py`
**è¡Œå·**: 64-67
ä½¿ç”¨ Cholesky åˆ†è§£æ¥è®¡ç®—é€†çŸ©é˜µä»¥ä¿è¯æ•°å€¼ç¨³å®šæ€§ï¼š
```python
damp = percdamp * torch.mean(torch.diag(H))
diag = torch.arange(self.columns, device=self.dev)
H[diag, diag] += damp
H = torch.linalg.cholesky(H)
H = torch.cholesky_inverse(H)
```

### æ˜¾è‘—æ€§åˆ†æ•° (OBS Metric)
**æ–‡ä»¶**: `lib/sparsegpt.py`
**è¡Œå·**: 84
åŸºäº Optimal Brain Surgeon ç†è®ºè®¡ç®—æƒé‡çš„é‡è¦æ€§åˆ†æ•°ï¼š
```python
tmp = W1 ** 2 / (torch.diag(Hinv1).reshape((1, -1))) ** 2
```

### å›°æƒ‘åº¦è®¡ç®— (Perplexity)
**æ–‡ä»¶**: `lib/eval.py`
**è¡Œå·**: 75
å°†æ‰€æœ‰æ‰¹æ¬¡çš„è´Ÿå¯¹æ•°ä¼¼ç„¶æ±‚å’Œåå–æŒ‡æ•°ï¼š
```python
ppl = torch.exp(torch.stack(nlls).sum() / (nsamples * model.seqlen))
```

### è´Ÿå¯¹æ•°ä¼¼ç„¶ (NLL)
**æ–‡ä»¶**: `lib/eval.py`
**è¡Œå·**: 66-70
è®¡ç®—å•ä¸ªæ‰¹æ¬¡çš„æŸå¤±å¹¶è½¬æ¢ä¸ºè´Ÿå¯¹æ•°ä¼¼ç„¶ï¼š
```python
loss = loss_fct(shift_logits.reshape(-1, shift_logits.size(-1)), shift_labels.reshape(-1))
neg_log_likelihood = loss.float() * model.seqlen * (j-i)
```

### Min-Max å½’ä¸€åŒ–ç®—å­
**æ–‡ä»¶**: `lib/gptree.py`
**è¡Œå·**: 99
å°†è¾“å…¥å¼ é‡ç¼©æ”¾åˆ° [0, 1] åŒºé—´ï¼š
```python
return (x - x.min()) / (x.max() - x.min())
```

### Z-Score å½’ä¸€åŒ–ç®—å­
**æ–‡ä»¶**: `lib/gptree.py`
**è¡Œå·**: 107
æ ‡å‡†åŒ–è¾“å…¥å¼ é‡ï¼Œä½¿å…¶å‡å€¼ä¸º0ï¼Œæ–¹å·®ä¸º1ï¼š
```python
return (x - x.mean()) / x.std()
```

### é™¤æ³•ç®—å­ (å¸¦å½’ä¸€åŒ–)
**æ–‡ä»¶**: `lib/gptree.py`
**è¡Œå·**: 37
Pruner-Zero ç‰¹å®šçš„ç®—å­è®¾è®¡ï¼Œåˆ†æ¯ä½¿ç”¨ L2 èŒƒæ•°ï¼š
```python
return x / torch.norm(y)
```

### å¯¹æ•°ç®—å­ (æ•°å€¼ç¨³å®š)
**æ–‡ä»¶**: `lib/gptree.py`
**è¡Œå·**: 70
å¢åŠ  epsilon (0.001) é˜²æ­¢ log(0) é”™è¯¯ï¼š
```python
return torch.log(torch.abs(x) + 0.001)
```

### æ¨¡å‹ç¨€ç–åº¦è®¡ç®—
**æ–‡ä»¶**: `lib/prune.py`
**è¡Œå·**: 49-58
ç»Ÿè®¡æ¨¡å‹ä¸­æƒé‡ä¸º 0 çš„æ¯”ä¾‹ï¼š
```python
count += (W==0).sum().item()
# ...
total_params += W.numel()
# ...
return float(count)/total_params
``` 

## 5. å®‰è£…ä¸ç¯å¢ƒ

Step 1: Create a new conda environment:
```
conda create -n pruner_zero python=3.9
conda activate pruner_zero
```

Step 2: Install relevant packages

```
conda install pytorch==1.10.1 torchvision==0.11.2 torchaudio==0.10.1 cudatoolkit=11.3 -c pytorch -c conda-forge
pip install transformers==4.28.0 datasets==2.11.0 wandb sentencepiece
pip install accelerate==0.18.0
```

## 6. æ•°æ®é›†å‡†å¤‡

### 6.1 æ•°æ®é›†åŠ è½½æ ¸å¿ƒä»£ç 

é¡¹ç›®ä½¿ç”¨äº†ä¸¤ä¸ªä¸»è¦æ•°æ®é›†ï¼š**WikiText-2** å’Œ **C4**ã€‚æ•°æ®åŠ è½½çš„æ ¸å¿ƒå®ç°ä½äº `lib/data. py`ï¼š

```python name=lib/data.py url=https://github.com/L-chen666/Pruner-Zero-1/blob/2f97f98a6ed99ad0c9137471b8fc04e72be071de/lib/data.py
# Code adapted from https://github.com/IST-DASLab/sparsegpt/blob/master/datautils.py

import numpy as np
import random
import torch
from datasets import load_dataset, load_from_disk 

# Set seed for reproducibility
def set_seed(seed):
    np.random.seed(seed)
    torch.random.manual_seed(seed)

# Wrapper for tokenized input IDs
class TokenizerWrapper:
    def __init__(self, input_ids):
        self.input_ids = input_ids

# Load and process wikitext2 dataset
def get_wikitext2(nsamples, seed, seqlen, tokenizer):
    # Load train and test datasets from local disk
    traindata = load_from_disk('./data/wikitext2_train')
    testdata = load_from_disk('./data/wikitext2_test')

    # Encode datasets
    trainenc = tokenizer(" ".join(traindata['text']), return_tensors='pt')
    testenc = tokenizer("\n\n".join(testdata['text']), return_tensors='pt')

    # Generate samples from training set
    random.seed(seed)
    trainloader = []
    for _ in range(nsamples):
        i = random.randint(0, trainenc.input_ids.shape[1] - seqlen - 1)
        j = i + seqlen
        inp = trainenc.input_ids[:, i:j]
        tar = inp.clone()
        tar[:, :-1] = -100
        trainloader.append((inp, tar))
    return trainloader, testenc

# Load and process c4 dataset
def get_c4(nsamples, seed, seqlen, tokenizer):
    # Load train and validation datasets from local disk
    traindata = load_from_disk('~/workspace/pruner-zero-private/data/c4_train')
    valdata = load_from_disk('~/workspace/pruner-zero-private/data/c4_valid')

    # Generate samples from training set
    random. seed(seed)
    trainloader = []
    for _ in range(nsamples):
        while True:
            i = random. randint(0, len(traindata) - 1)
            trainenc = tokenizer(traindata[i]['text'], return_tensors='pt')
            if trainenc. input_ids.shape[1] > seqlen:
                break
        i = random.randint(0, trainenc.input_ids.shape[1] - seqlen - 1)
        j = i + seqlen
        inp = trainenc.input_ids[:, i:j]
        tar = inp.clone()
        tar[:, :-1] = -100
        trainloader.append((inp, tar))

    # Prepare validation dataset
    valenc = tokenizer(' '.join(valdata[:1100]['text']), return_tensors='pt')
    valenc = valenc.input_ids[:, :(256 * seqlen)]
    valenc = TokenizerWrapper(valenc)
    return trainloader, valenc

# Function to select the appropriate loader based on dataset name
def get_loaders(name, nsamples=128, seed=0, seqlen=2048, tokenizer=None):
    if 'wikitext2' in name:
        return get_wikitext2(nsamples, seed, seqlen, tokenizer)
    if "c4" in name:
        return get_c4(nsamples, seed, seqlen, tokenizer)
```

### 6.2 æ•°æ®é›†å‡†å¤‡æ­¥éª¤

**WikiText-2 æ•°æ®é›†**ï¼š
- ä»æœ¬åœ°è·¯å¾„åŠ è½½ï¼š`./data/wikitext2_train` å’Œ `./data/wikitext2_test`
- æˆ–ä» HuggingFace åŠ è½½ï¼š`load_dataset('wikitext', 'wikitext-2-raw-v1')`

**C4 æ•°æ®é›†**ï¼š
- ä»æœ¬åœ°è·¯å¾„åŠ è½½ï¼š`~/workspace/pruner-zero-private/data/c4_train` å’Œ `~/workspace/pruner-zero-private/data/c4_valid`
- æˆ–ä» HuggingFace åŠ è½½ï¼š`load_dataset('allenai/c4')`

### 6.3 æ¢¯åº¦è®¡ç®—çš„æ•°æ®åŠ è½½

æ¢¯åº¦è®¡ç®—ä½¿ç”¨çš„æ˜¯ WikiText-2 æ•°æ®é›†ï¼Œä»£ç ä½äº `lib/gradient_computation.py`ï¼š

```python name=lib/gradient_computation.py url=https://github.com/L-chen666/Pruner-Zero-1/blob/2f97f98a6ed99ad0c9137471b8fc04e72be071de/lib/gradient_computation.py#L47-L67
def get_wikitext2(nsamples, seed, seqlen, tokenizer):
    # Load train and test datasets from local disk
    traindata = load_from_disk('./data/wikitext2_train')
    testdata = load_from_disk('./data/wikitext2_test')

    # Encode datasets
    trainenc = tokenizer(' '.join(traindata['text']), return_tensors='pt')
    testenc = tokenizer('\n\n'.join(testdata['text']), return_tensors='pt')

    # Generate samples from training set
    random.seed(seed)
    trainloader = []
    for _ in range(nsamples):
        i = random.randint(0, trainenc.input_ids.shape[1] - seqlen - 1)
        j = i + seqlen
        inp = trainenc.input_ids[:, i:j]
        tar = inp.clone()
        trainloader.append((inp, tar))
    return trainloader, testenc
```

---

## 7. å‘½ä»¤è¡Œå‚æ•°é…ç½®

### 7.1 ä¸»å‰ªæè„šæœ¬å‚æ•°é…ç½® (`main.py`)

```python name=main.py url=https://github.com/L-chen666/Pruner-Zero-1/blob/2f97f98a6ed99ad0c9137471b8fc04e72be071de/main.py#L33-L58
def main():
    parser = argparse. ArgumentParser()
    parser.add_argument('--model', type=str, help='LLaMA model')
    parser.add_argument('--seed', type=int, default=0, help='Seed for sampling the calibration data.')
    parser.add_argument('--nsamples', type=int, default=128, help='Number of calibration samples.')
    parser.add_argument('--sparsity_ratio', type=float, default=0, help='Sparsity level')
    parser.add_argument("--sparsity_type", type=str, choices=["unstructured", "4:8", "2:4"])
    parser.add_argument("--prune_method", type=str, 
                        choices=["magnitude", "wanda", "sparsegpt", 
                                "ablate_mag_seq", "ablate_wanda_seq", 
                                "ablate_mag_iter", "ablate_wanda_iter", 
                                "search", "pruner-zero", 
                                "ablate_prunerzero_seq", "ablate_prunerzero_iter"])
    parser.add_argument("--cache_dir", default="llm_weights", type=str)
    parser.add_argument('--use_variant', action="store_true", 
                       help="whether to use the wanda variant described in the appendix")
    parser.add_argument('--save', type=str, default=None, help='Path to save results.')
    parser.add_argument('--save_model', type=str, default=None, 
                       help='Path to save the pruned model.')
    parser.add_argument("--gradient_path", type=str, default=None, 
                       help="Path to save the gradient.")
    parser.add_argument("--json_tree", type=str, default="data/best_tree.json", 
                       help="Path to load the json tree.")
    parser.add_argument("--eval_zero_shot", action="store_true")
    args = parser.parse_args()
```

**ä¸»è¦å‚æ•°è¯´æ˜ï¼š**

| å‚æ•°å | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|--------|------|--------|------|
| `--model` | str | - | HuggingFace æ¨¡å‹è·¯å¾„æˆ–åç§°ï¼ˆå¦‚ `meta-llama/Llama-2-7b-hf`ï¼‰ |
| `--seed` | int | 0 | éšæœºç§å­ |
| `--nsamples` | int | 128 | æ ¡å‡†æ•°æ®æ ·æœ¬æ•°é‡ |
| `--sparsity_ratio` | float | 0 | ç¨€ç–åº¦æ¯”ä¾‹ï¼ˆ0-1ï¼‰ |
| `--sparsity_type` | str | - | ç¨€ç–åº¦ç±»å‹ï¼š`unstructured`ã€`2:4`ã€`4:8` |
| `--prune_method` | str | - | å‰ªææ–¹æ³•ï¼š`pruner-zero`ã€`wanda`ã€`magnitude` ç­‰ |
| `--cache_dir` | str | `llm_weights` | æ¨¡å‹æƒé‡ç¼“å­˜ç›®å½• |
| `--save` | str | None | ç»“æœä¿å­˜è·¯å¾„ |
| `--save_model` | str | None | å‰ªæåæ¨¡å‹ä¿å­˜è·¯å¾„ |
| `--gradient_path` | str | None | æ¢¯åº¦æ–‡ä»¶è·¯å¾„ï¼ˆPruner-Zero å¿…éœ€ï¼‰ |
| `--json_tree` | str | `data/best_tree.json` | ç¬¦å·æ ‘ JSON æ–‡ä»¶è·¯å¾„ |
| `--eval_zero_shot` | flag | False | æ˜¯å¦è¿›è¡Œé›¶æ ·æœ¬è¯„ä¼° |

### 7.2 OPT æ¨¡å‹å‰ªæå‚æ•°é…ç½® (`main_opt.py`)

```python name=main_opt.py url=https://github.com/L-chen666/Pruner-Zero-1/blob/2f97f98a6ed99ad0c9137471b8fc04e72be071de/main_opt.py#L31-L47
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--model', type=str, help='LLaMA model')
    parser.add_argument('--seed', type=int, default=0, help='Seed for sampling the calibration data.')
    parser.add_argument('--nsamples', type=int, default=128, help='Number of calibration samples.')
    parser.add_argument('--sparsity_ratio', type=float, default=0, help='Sparsity level')
    parser.add_argument("--sparsity_type", type=str, choices=["unstructured", "4:8", "2:4"])
    parser.add_argument("--prune_method", type=str, 
                        choices=["magnitude", "wanda", "sparsegpt", 
                                "ablate_mag_seq", "ablate_wanda_seq", 
                                "ablate_mag_iter", "ablate_wanda_iter", 
                                "search", "pruner-zero"])
    parser.add_argument("--cache_dir", default="llm_weights", type=str)
    parser.add_argument('--use_variant', action="store_true", 
                       help="whether to use the wanda variant described in the appendix")
    parser.add_argument('--save', type=str, default=None, help='Path to save results.')
    parser.add_argument('--save_model', type=str, default=None, 
                       help='Path to save the pruned model.')
    parser.add_argument("--gradient_path", type=str, default=None, 
                       help="Path to save the gradient.")
    parser. add_argument("--eval_zero_shot", action="store_true")
```

### 7.3 æ¢¯åº¦è®¡ç®—å‚æ•°é…ç½® (`lib/gradient_computation.py`)

```python name=lib/gradient_computation.py url=https://github.com/L-chen666/Pruner-Zero-1/blob/2f97f98a6ed99ad0c9137471b8fc04e72be071de/lib/gradient_computation.py#L198-L212
if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--nsamples', type=int, default=2, help='no of samples used')
    parser.add_argument('--scale', type=int, default=100, help='scale factor for gradient')
    parser.add_argument('--llama_version', type=int, default=2, help='llama version used')
    parser.add_argument('--model', type=str, help='model to used')
    parser.add_argument('--task', type=str, default='gradient', 
                       help='task to be performed (gradient or activation)')
    parser.add_argument('--seed', type=int, default=0, help='seed used')
```

### 7.4 LoRA å¾®è°ƒå‚æ•°é…ç½® (`lora_ft/finetune_lm.py`)

```python name=lora_ft/finetune_lm.py url=https://github.com/L-chen666/Pruner-Zero-1/blob/2f97f98a6ed99ad0c9137471b8fc04e72be071de/lora_ft/finetune_lm.py#L251-L267
def main():
    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))
    if len(sys.argv) == 2 and sys.argv[1]. endswith(". json"):
        # If we pass only one argument to the script and it's the path to a json file,
        # let's parse it to get our arguments. 
        model_args, data_args, training_args = parser. parse_json_file(
            json_file=os.path.abspath(sys.argv[1]))
    else:
        model_args, data_args, training_args = parser.parse_args_into_dataclasses()
```

**LoRA å¾®è°ƒæ•°æ®å‚æ•°ï¼š**

```python
@dataclass
class DataTrainingArguments:
    dataset_name: Optional[str] = field(default=None)  # æ•°æ®é›†åç§°
    dataset_config_name: Optional[str] = field(default=None)  # æ•°æ®é›†é…ç½®
    train_file: Optional[str] = field(default=None)  # è®­ç»ƒæ–‡ä»¶è·¯å¾„
    validation_file: Optional[str] = field(default=None)  # éªŒè¯æ–‡ä»¶è·¯å¾„
    max_train_samples: Optional[int] = field(default=None)  # æœ€å¤§è®­ç»ƒæ ·æœ¬æ•°
    max_eval_samples: Optional[int] = field(default=None)  # æœ€å¤§è¯„ä¼°æ ·æœ¬æ•°
    block_size: Optional[int] = field(default=1024)  # ä¸Šä¸‹æ–‡é•¿åº¦
    preprocessing_num_workers: Optional[int] = field(default=None)  # é¢„å¤„ç†è¿›ç¨‹æ•°
    validation_split_percentage: Optional[int] = field(default=5)  # éªŒè¯é›†æ¯”ä¾‹
```

**LoRA æ¨¡å‹å‚æ•°ï¼š**

```python
@dataclass
class ModelArguments:
    model_name_or_path: Optional[str]  # æ¨¡å‹è·¯å¾„
    lora_r: Optional[int] = field(default=8)  # LoRA rank
    lora_alpha: Optional[int] = field(default=16)  # LoRA alpha
    lora_dropout: Optional[float] = field(default=0.05)  # LoRA dropout
```

---

## 8. å®Œæ•´è¿è¡Œå‘½ä»¤ç¤ºä¾‹

### 8.1 æ¢¯åº¦è®¡ç®—å‘½ä»¤

```bash
CUDA_VISIBLE_DEVICES=0 python lib/gradient_computation.py \
    --nsamples 128 \
    --scale 100 \
    --model meta-llama/Llama-2-7b-hf \
    --llama_version 2 \
    --task gradient \
    --seed 0
```

### 8.2 éç»“æ„åŒ–å‰ªæå‘½ä»¤ï¼ˆ50% ç¨€ç–åº¦ï¼‰

```bash
python main.py \
    --model meta-llama/Llama-2-7b-hf \
    --prune_method pruner-zero \
    --sparsity_ratio 0.5 \
    --sparsity_type unstructured \
    --nsamples 128 \
    --seed 0 \
    --gradient_path data/grad_llama2_7b.pt \
    --json_tree data/best_tree. json \
    --save out/llama_7b/unstructured/pruner-zero/ \
    --cache_dir llm_weights
```

### 8.3 ç»“æ„åŒ–å‰ªæå‘½ä»¤ï¼ˆ2:4 ç¨€ç–åº¦ï¼‰

```bash
python main. py \
    --model meta-llama/Llama-2-7b-hf \
    --prune_method pruner-zero \
    --sparsity_ratio 0.5 \
    --sparsity_type 2:4 \
    --nsamples 128 \
    --gradient_path data/grad_llama2_7b.pt \
    --json_tree data/best_tree.json \
    --save out/llama_7b/2-4/pruner-zero/
```

### 8.4 OPT æ¨¡å‹å‰ªæå‘½ä»¤

```bash
python main_opt.py \
    --model facebook/opt-6.7b \
    --prune_method pruner-zero \
    --sparsity_ratio 0.5 \
    --sparsity_type unstructured \
    --nsamples 128 \
    --gradient_path data/grad_opt_6.7b.pt \
    --save out/opt_6.7b/unstructured/pruner-zero/
```

### 8.5 LoRA å¾®è°ƒå‘½ä»¤

```bash
CUDA_VISIBLE_DEVICES=0 python lora_ft/finetune_lm.py \
    --model_name_or_path out/llama_7b/unstructured/pruner-zero/ \
    --config_name meta-llama/Llama-2-7b-hf \
    --dataset_name c4 \
    --num_train_epochs 1 \
    --block_size 1024 \
    --per_device_train_batch_size 1 \
    --per_device_eval_batch_size 8 \
    --do_train \
    --do_eval \
    --max_train_samples 30000 \
    --max_eval_samples 128 \
    --learning_rate 1e-4 \
    --overwrite_output_dir \
    --output_dir out/llama_7b_lora/
```

### 8.6 LoRA æ¨¡å‹è¯„ä¼°å‘½ä»¤

```bash
python lora_ft/evaluate_ppl.py \
    --model out/llama_7b/unstructured/pruner-zero/ \
    --lora_weights out/llama_7b_lora/ \
    --cache_dir llm_weights \
    --ctx_length 2048 \
    --eval_zero_shot
```

### 8.7 é›¶æ ·æœ¬è¯„ä¼°å‘½ä»¤

```bash
python main.py \
    --model meta-llama/Llama-2-7b-hf \
    --prune_method pruner-zero \
    --sparsity_ratio 0.5 \
    --sparsity_type unstructured \
    --gradient_path data/grad_llama2_7b.pt \
    --json_tree data/best_tree. json \
    --save out/llama_7b/unstructured/pruner-zero/ \
    --eval_zero_shot
```

---

## 9. æ•°æ®åŠ è½½è°ƒç”¨æµç¨‹

### 9.1 å‰ªææ—¶çš„æ•°æ®åŠ è½½

åœ¨ `lib/prune. py` ä¸­çš„ `prune_pruner_zero` å‡½æ•°ï¼š

```python
print("loading calibdation data")
dataloader, _ = get_loaders(
    "c4",
    nsamples=args.nsamples,
    seed=args.seed,
    seqlen=model. seqlen,
    tokenizer=tokenizer
)
print("dataset loading complete")
```

### 9.2 è¯„ä¼°æ—¶çš„æ•°æ®åŠ è½½

åœ¨ `lib/eval.py` ä¸­çš„ `eval_ppl` å‡½æ•°ï¼š

```python
def eval_ppl(args, model, tokenizer, device=torch.device("cuda:0")):
    dataset = "wikitext2"
    print(f"evaluating on {dataset}")
    
    # Get the test loader
    _, testloader = get_loaders(
        dataset, 
        seed=0, 
        seqlen=model.seqlen, 
        tokenizer=tokenizer
    )
    
    with torch.no_grad():
        ppl_test = eval_ppl_wikitext(model, testloader, 1, device)
    return ppl_test
```

---

## 10.è¿è¡Œç»“æœ

ç”±äºæ•°æ®é›†è¿‡å¤§ï¼Œå› æ­¤åªåšäº†Pruner-Zeroåœ¨ LLaMA-7B æ¨¡å‹è¿›è¡Œ 50% éç»“æ„åŒ–å‰ªæ,å¹¶ç»“åˆè‡ªå·±ç ”ç©¶æ–¹å‘åšå…¶ä»–æ–¹æ³•çš„å¯¹æ¯”å®éªŒï¼Œæ‰€æœ‰çš„å®éªŒå‡åœ¨ç›¸åŒçš„ç¯å¢ƒä¸‹è¿è¡Œï¼Œå¹¶ä½¿ç”¨äº†ä»¥ä¸‹ç»Ÿä¸€å‚æ•°ï¼š

*   **æ¨¡å‹ (Model)**: `llama_7b`
*   **ç¨€ç–åº¦ (Sparsity Ratio)**: `0.5` (å‰ªé™¤ 50% çš„å‚æ•°)
*   **ç¨€ç–ç±»å‹ (Sparsity Type)**: `unstructured` (éç»“æ„åŒ–å‰ªæ)
*   **è¯„ä¼°æ•°æ®é›† (Dataset)**: `wikitext2`
*   **è¯„ä¼°æŒ‡æ ‡ (Metric)**: Perplexity (å›°æƒ‘åº¦ï¼ŒPPL) - **æ•°å€¼è¶Šä½è¶Šå¥½**

### å›¾ 1ï¼šPruner-Zero æ–¹æ³• 

<p align="center">
<img src="https://github.com/L-chen666/Pruner-Zero-1/blob/main/Pruner-Zero-test.png" width=100% height=100% 
class="center">
 
*   **è¿è¡Œå‘½ä»¤**: 
    ```bash
    python main.py ... --prune_method pruner-zero ... --json_tree .../best_tree.json
    ```
*   **æ–¹æ³•ç®€ä»‹**: ä½¿ç”¨è¯¥é¡¹ç›®æå‡ºçš„ Pruner-Zero ç®—æ³•ï¼Œä¾èµ–äºè¿›åŒ–çš„ç¬¦å·å…¬å¼ç”Ÿæˆçš„å†³ç­–æ ‘ (`best_tree.json`) è¿›è¡Œå‰ªæã€‚
*   **è¿è¡Œç»“æœ**: `wikitext perplexity 6.876140594482422`
*   **åˆ†æ**: 
    *   **PPL: 6.88**
    *   è¿™æ˜¯ Pruner-Zero ç®—æ³•çš„å®é™…è¡¨ç°ã€‚åœ¨åŒç­‰ç¨€ç–åº¦ä¸‹ï¼Œå®ƒæˆåŠŸå°†æ¨¡å‹çš„å›°æƒ‘åº¦æ§åˆ¶åœ¨å¾ˆä½çš„æ°´å¹³ã€‚

### å›¾ 2ï¼šSparseGPT æ–¹æ³• 

<p align="center">
<img src="https://github.com/L-chen666/Pruner-Zero-1/blob/main/SparseGPT-test.png" width=100% height=100% 
class="center">
 
*   **è¿è¡Œå‘½ä»¤**:
    ```bash
    python main.py ... --prune_method sparsegpt ...
    ```
*   **æ–¹æ³•ç®€ä»‹**: SparseGPT æ˜¯ä¸€ç§ç»å…¸çš„åŸºäºäºŒé˜¶ Hessian ä¿¡æ¯çš„å‰ªæç®—æ³•ï¼Œé€šå¸¸ä½œä¸ºè¯¥é¢†åŸŸçš„ SOTA (State-of-the-Art) åŸºçº¿è¿›è¡Œå¯¹æ¯”ã€‚
*   **è¿è¡Œç»“æœ**: `wikitext perplexity 6.72606086730957`
*   **åˆ†æ**: 
    *   **PPL: 6.73** (æœ¬æ¬¡æµ‹è¯•ä¸­çš„**æœ€ä¼˜ç»“æœ**)
    *   åœ¨è¿™ä¸ªç‰¹å®šçš„è®¾ç½®ä¸‹ï¼ˆ50% éç»“æ„åŒ–ç¨€ç–ï¼‰ï¼ŒSparseGPT è¡¨ç°ç•¥å¾®ä¼˜äº Pruner-Zeroã€‚è¿™è¡¨æ˜åˆ©ç”¨äºŒé˜¶ä¿¡æ¯å¯¹äºä¿ç•™æ¨¡å‹ç²¾åº¦éå¸¸æœ‰æ•ˆã€‚

### å›¾ 3ï¼šWanda æ–¹æ³• 

<p align="center">
<img src="https://github.com/L-chen666/Pruner-Zero-1/blob/main/Wanda-test.png" width=100% height=100% 
class="center">
 
*   **è¿è¡Œå‘½ä»¤**:
    ```bash
    python main.py ... --prune_method wanda ...
    ```
*   **æ–¹æ³•ç®€ä»‹**: Wanda (Pruning by Weights and activations) æ˜¯ä¸€ç§åŸºäºæƒé‡å¹…åº¦å’Œè¾“å…¥æ¿€æ´»å€¼ä¹˜ç§¯çš„å‰ªææ–¹æ³•ï¼Œè®¡ç®—é‡é€šå¸¸å°äº SparseGPTã€‚
*   **è¿è¡Œç»“æœ**: `wikitext perplexity 7.091907501220703`
*   **åˆ†æ**: 
    *   **PPL: 7.09**
    *   åœ¨æœ¬æ¬¡å¯¹æ¯”ä¸­è¡¨ç°æœ€å¼±ã€‚ç›¸æ¯”äºå‰ä¸¤ç§æ–¹æ³•ï¼ŒWanda åœ¨ 50% ç¨€ç–åº¦ä¸‹çš„ç²¾åº¦æŸå¤±æœ€å¤§ã€‚

| æ’å | æˆªå›¾ç¼–å· | å‰ªææ–¹æ³• (Method) | å›°æƒ‘åº¦ (PPL) | ç›¸å¯¹è¡¨ç° |
| :--- | :--- | :--- | :--- | :--- |
| **1** | å›¾ 2 | **SparseGPT** | **6.73** |  **æœ€ä¼˜** (ä¿ç•™èƒ½åŠ›æœ€å¼º) |
| 2 | å›¾ 1 | Pruner-Zero | 6.88 |  æ¬¡ä¼˜ (éå¸¸æ¥è¿‘ SparseGPT) |
| 3 | å›¾ 3 | Wanda | 7.09 |  è¾ƒå·® |

**ç»“è®º**: 
åœ¨ LLaMA-7B æ¨¡å‹è¿›è¡Œ 50% éç»“æ„åŒ–å‰ªæçš„ä»»åŠ¡ä¸Šï¼Œ**SparseGPT æ•ˆæœæœ€å¥½**ï¼ŒPruner-Zero ç´§éšå…¶åï¼ˆå·®è·ä»…çº¦ 0.15 PPLï¼‰ï¼Œè€Œ Wanda çš„æ•ˆæœç›¸å¯¹è¾ƒå·®ã€‚è¿™éªŒè¯äº†ä»£ç åº“èƒ½å¤Ÿæ­£ç¡®å¤ç°ä¸åŒ Baseline çš„æ€§èƒ½ï¼Œå¹¶æä¾›äº†æœ‰æ•ˆçš„å¯¹æ¯”æ•°æ®ã€‚
